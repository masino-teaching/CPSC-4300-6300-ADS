{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='heading'>\n",
    "    <div style='float:left;'><h1>CPSC 4300/6300: Applied Data Science</h1></div>\n",
    "     <img style=\"float: right; padding-right: 10px\" width=\"100\" src=\"https://raw.githubusercontent.com/bsethwalker/clemson-cs4300/main/images/clemson_paw.png\"> </div>\n",
    "     </div>\n",
    "\n",
    "**Clemson University**<br>\n",
    "**Instructor(s):** Aaron Masino <br>\n",
    "\n",
    "## Lab 6: Classification With Logistic Regression and Tree Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import load_wine, load_iris\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, classification_report, confusion_matrix,  ConfusionMatrixDisplay, RocCurveDisplay, PrecisionRecallDisplay\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 654321"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Goals\n",
    "\n",
    "By the end of this lab, you should be able to:\n",
    "- Apply the scikit-learn LogisticRegression, DecisionTreeClassifier, RandomForrestClassifier modules to learn classification models from data \n",
    "- Apply the scikit-learn metrics (e.g., confustion_matrix) to trained classifier models\n",
    "- Analyze classification metrics to compare different models and assess performance\n",
    "- Apply the scikit-learn GridSearchCV and model selection principles to identify select the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 Wine Data Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wine Data Exploration\n",
    "Let's load the _wine_ dataset and briefly explore some of its characteristics. The _wine_ dataset is a _toy_ dataset that is available in the scikit-learn. We can load the dataset using scikit-learn's `load_wine` method that will create an sklearn `Bunch` object. Although we can work with this object directly in scikit-learn models, we'll convert it to our more familiar Pandas DataFrame. \n",
    "\n",
    "For more information see, [scikit-learn toy datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the wine dataset from sklearn\n",
    "wine = load_wine()\n",
    "\n",
    "# the wine object is a dictionary-like object NOT a pandas dataframe\n",
    "print(wine.DESCR)\n",
    "\n",
    "# we can work with the wine object directly, but let's convert it to a pandas dataframe to be consistent with our prior examples\n",
    "df_wine = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "df_wine['target'] = wine.target\n",
    "display(df_wine.head())\n",
    "print(df_wine.target.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the distribution of the variables in the dataset. We will examine box plots of the different features when grouped by the target variable. Ideally, we would like to see that the distributions are different across the different values of the target variable (e.g., have different means or variance). Although such univariate differences are not a necessary condition for the features to be useful in a classification model, they are usually a sufficient condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_box_grid_by_target(data, target_label, num_cols = 4, fig_size = (20, 20), add_xlabel = True, label_fontsize = 20):\n",
    "    l = data.columns.tolist()\n",
    "    l.remove(target_label)\n",
    "    num_rows = -(-len(l) // num_cols)\n",
    "    fig, ax = plt.subplots(num_rows, num_cols, figsize=fig_size, squeeze=False)\n",
    "    row = 0\n",
    "    col = 0\n",
    "    cnt = 0\n",
    "    for c in l:\n",
    "        sns.boxplot(y=c, hue=target_label, ax=ax[row, col], data=data)\n",
    "        if add_xlabel:\n",
    "            ax[row, col].set_xlabel(data.columns[cnt], fontsize=label_fontsize)\n",
    "        cnt += 1\n",
    "        if cnt % num_cols == 0:\n",
    "            row += 1\n",
    "            col = 0\n",
    "        else:\n",
    "            col += 1\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    return fig, ax\n",
    "\n",
    "plt_box_grid_by_target(df_wine, 'target', num_cols=4, fig_size=(20, 20));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that the many of the variables have different distributions when segmented by the target label, suggests we should be able to use these features to create a good performance classification model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Sklearn Classifiers with Wine Data \n",
    "Let's first introduce some of the many classifier models available in scikit-learn. For now, we just want to see how we use the data to create these models, get the parameters, and evaluate the results. In later sections, we'll address using a systematic approach for model selection. We'll consider three specific classification models:\n",
    "- Logistic Regression\n",
    "- Decision Tree\n",
    "- Random Forest \n",
    "\n",
    "There are many, many more. For a complete review of all of the supervised learning (regression and classification) models available in scikit-learn, see [here](https://scikit-learn.org/stable/supervised_learning.html).\n",
    "\n",
    "### Logistic Regression with Sklearn\n",
    "We'll begin with fitting a logistic regression model using the scikit-learn `LogisticRegression` module (see documenation [here](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)). \n",
    "\n",
    "As we've seen previously with sklearn models, the standard pattern for training a classification model using sklearn modules is:\n",
    "```\n",
    "my_model = Some_SKLearn_Classifier(arg_1=value, ...)\n",
    "my_model.fit(my_X_data, my_y_data)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Logistic Regression model\n",
    "# we set penalty=None to avoid regularization\n",
    "model_lr = LogisticRegression(random_state=SEED, penalty=None, solver='lbfgs')\n",
    "\n",
    "# fit the model to our wine data - we should be using a training set, but we'll get to that later\n",
    "# we'll use all the features as our X and the target as our y\n",
    "model_lr.fit(df_wine.drop('target', axis=1), df_wine.target)\n",
    "\n",
    "# get the coefficients of the model\n",
    "# print the coefficients with the feature names\n",
    "print('Intercept:', model_lr.intercept_[0])\n",
    "print('\\nCoefficients:')\n",
    "for i, c in enumerate(model_lr.coef_[0]):\n",
    "    print(f'{df_wine.columns[i]}: {c:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the confusion matrix. Remember, we're looking at `training set` performance here, so the results are likely to be optimistic. We'll address this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# first we need the model predictions \n",
    "predictions = model_lr.predict(df_wine.drop('target', axis=1))\n",
    "\n",
    "# next we can use the confusion_matrix function to get the confusion matrix\n",
    "cm = confusion_matrix(df_wine.target, predictions)\n",
    "\n",
    "# we can use the ConfusionMatrixDisplay to plot the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=[f'Class {i}' for i in range(3)])\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title('Confusion Matrix For Wine Data Using Logistic Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also look at some of the standard performance metrics. There are many performance metrics avaialable in the sklearn libary (see [documentation](https://scikit-learn.org/stable/modules/model_evaluation.html#)). We will use the `classification_report` function, which provides a summary table of the _intra class_ performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(df_wine.target, predictions, target_names=[f'Class {i}' for i in range(3)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep the table small and accomodate multiclass scenarios, scikit-learn presents _intraclass_ metrics. Let's break these down sarting with the first two rows. Let _P_ be the number of actual postive samples, _N_ be the number of actual negative samples, PP be the number of predicted positive, PN be the number of predicted negative, _TP_ be the number of true postives (correctly predicted postives), _FP_ be the number of false postives, let _TN_ be the number of true negatives (correctly predicted negatives), and _FN_ be the number of false negatives __within the given class__ (i.e., only the samples in the class are considered, the number of samples in the class is given by the `support` column):\n",
    "- __class 0 precision ($C_0P$)__: equal to _TP/PP=TP_/(_TP_+_FP_) also known as _positive predictive value_ (PPV)\n",
    "- __class 0 recall ($C_0R$)__: equal to _TP/P=TP_/(_TP_+_FN_) also known as _sensitivity_\n",
    "- __class 0 f1-score__ : equal to $2\\frac{C_0P\\times C_0R}{C_0P+C_0R}$\n",
    "- __class 1 precision ($C_1P$)__: equal to _TP/P=TP_/(_TP_+_FP_)  \n",
    "- __class 1 recall ($C_1R$)__: equal to _TP/P=TP_/(_TP_+_FN_) \n",
    "- __class 1 f1-score__ : equal to $2\\frac{C_1P\\times C_1R}{C_1P+C_1R}$\n",
    "- __class 2 precision ($C_2P$)__: equal to _TP/P=TP_/(_TP_+_FP_)  \n",
    "- __class 2 recall ($C_2R$)__: equal to _TP/P=TP_/(_TP_+_FN_) \n",
    "- __class 2 f1-score__ : equal to $2\\frac{C_2P\\times C_2R}{C_2P+C_2R}$\n",
    "\n",
    "The _accuracy_ row is simply the overall model accuracy = (TP+TN)/(P+N) taken across all samples\n",
    "\n",
    "The _macro avg_ row is the unweighted average over the class metric scores for each class. For example, the _macro avg precision_ = $\\frac{1}{3}(C_0P + C_1P + C_2P$). The _macro avg_ treats all classes equally.\n",
    "\n",
    "The _weighted avg_ row is similar to to the _macro _avg_ but weights each class metric score by the support proportion. For example, the _macro avg precison_ = $\\frac{1}{178}(59C_0P + 71C_1P + 48C_2P)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier\n",
    "Now let's see how to build a decision tree classifier with scikit-learn. We'll use the [DecisiionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dt = DecisionTreeClassifier(random_state=SEED)\n",
    "model_dt.fit(df_wine.drop('target', axis=1), df_wine.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the confusion matrix for the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_dt.predict(df_wine.drop('target', axis=1))\n",
    "cm = confusion_matrix(df_wine.target, predictions)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=[f'Class {i}' for i in range(3)])\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title('Confusion Matrix For Wine Data Using Decision Tree')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, the model got every __training__ example correct. Is this a good thing? We should be concerned about overfitting. Let's take a look at the decision tree. The depth of the tree might provide some information regarding the potential of overfitting. We can use the scikit-learn [plot_tree](https://scikit-learn.org/1.5/modules/generated/sklearn.tree.plot_tree.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,16))\n",
    "plot_tree(model_dt, feature_names=df_wine.columns[:-1], class_names=[f'Class {i}' for i in range(3)], filled=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems like a fairly complex tree for a dataset with only 178 training samples. We should definitely be concerned about overfitting. We'll address this later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier\n",
    "Finally, let's see how to build a random forest classifier with scikit-learn. Recall, a random forest is an ensemble classifier that builds many decision trees using a bagging approach wherein multiple training samples are created with bootstrapping. We'll use the [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = RandomForestClassifier(random_state=SEED)\n",
    "model_rf.fit(df_wine.drop('target', axis=1), df_wine.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again look at the confusion matrix to see how the model did on the _training_ data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_rf.predict(df_wine.drop('target', axis=1))\n",
    "cm = confusion_matrix(df_wine.target, predictions)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=[f'Class {i}' for i in range(3)])\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title('Confusion Matrix For Wine Data Using Random Forest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wine Data Model Selection with GridSearch and Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have been training individual models using all of the data. There are several problems with this approach. First, we have not used a held out test set which means we only have performance evaluation on the training data which can be overly optimisitic (remember bias and overfitting are always a concern). Additionally, we all of our models have one or more hyperparameters. Recall, hyperparameters are used to tune model performance (e.g., regularization) but are not learned from the data. We need a principled approach to select the best model and hyperparameter combination. \n",
    "\n",
    "For this example, let's put all the tools that we've learned together and apply them in a systematic model selection and evaluation process. We will want to complete the following steps:\n",
    "1. Check for missing data\n",
    "2. Split our data into a training and test set\n",
    "3. Standardize the data using the mean and variance from the training data.\n",
    "4. Select our model classes and corresponding hyperparameter grids\n",
    "5. Apply cross validation to find the best model and hyperparameter combination and then retrain the best combination on all training data \n",
    "6. Evaluate the best model on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Check for missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the wine dataset for missing values\n",
    "print(df_wine.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have any missing data in this case. If we did, we would need to assess if it was missing completely at random, missing at random, or missing not at random. From there, we could determine if we should impute missing data or drop certain features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Train and test set split\n",
    "\n",
    "As always, we need to split our data into training and test samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's shuffle the wine data\n",
    "df_wine = df_wine.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "# let's split the df_wine data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_wine.drop('target', axis=1), df_wine.target, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Standardize the feature values\n",
    "\n",
    "Now let's standardize our continuous features. Our features are all continuous. However, if we had qualitative feature variables, we would also need to convert those to dummy variables in this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's standardize the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit the scaler to the training data\n",
    "scaler.fit(X_train)\n",
    "# transform the training data and put it into a new dataframe\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "\n",
    "# transform the testing data and put it into a new dataframe\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "# show the scaled training data\n",
    "display(X_train_scaled.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Define the model & hyperparameter grid options (THIS PART IS NEW!!)\n",
    "\n",
    "We now need to decide which models we want to train and compare. Here we are again considering logistic regression, decision tree, and random forest classifiers. These are subjective choices and in different scenarios, we might select other types of classifier models. \n",
    "\n",
    "Each of the models we are considering has different hyperparameters. For example, logistic regression has the parameter `C` which is the _inverse regularization_ parameter ($C=1/\\lambda$) which is the reciprocal of the regularization parameter we've seen previously. The decision tree classifier has different parametrs, one of which is `max_depth` which specifies the maximum number of splits allowed. The random forest has `n_estimators` which specifies the number of trees to build and the `max_depth` for each tree. Each of these models actually has additional hyperparameters (refer to the documentation.) To evaluate the hyperparameters, we will specifiy a dictionary that indicates the values we want to test for each parameter. For models with more than one hyperparameter, we will be testing the cross product of the specified values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model dictionary\n",
    "models = {\n",
    "    'logistic_regression': LogisticRegression(random_state=SEED, max_iter=1000, tol=0.001),\n",
    "    'decision_tree': DecisionTreeClassifier(random_state=SEED),\n",
    "    'random_forest': RandomForestClassifier(random_state=SEED)\n",
    "}\n",
    "\n",
    "# create a hyperparameter dictionary for the models\n",
    "param_grid = {'logistic_regression':{ 'C': [0.1, 1, 10],\n",
    "                                      'penalty': ['l1', 'l2',\n",
    "                                    'elasticnet'],'solver': ['saga']}, # we need to use the saga solver for l1 and elastic net regularization\n",
    "              'decision_tree': {'max_depth': [None, 3, 5]},\n",
    "              'random_forest': {'n_estimators': [10, 50, 100],\n",
    "                                'max_depth': [None, 3, 5]}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Grid search with cross-validation & final model training (THIS PART IS NEW!!)\n",
    "Now we want to train each model with each combination of hyperparameters and evaluate them to find the best performing one. However, we don't want to evaluate all of these models on the test set (why?). Instead, we will apply cross validation for __every__ model and hyperparameter combination. This will provide an average cross-validation score for each model and hyperparameter combination. \n",
    "\n",
    "Our __final selection__ will be the model and hyperparameter combination with the best cross-validation performance. To obtain a __final model__, we will use the best model class and hyperparameter combination and train it on the entire _training_ set. Fortunately, the scikit-learn [GridSearchCV](https://scikit-learn.org/dev/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV.score_samples) module does most of the work for us. For a given model class (e.g., `LogisticRegressionClassifier`), the `GridSearchCV` will perform cross validation for each of the specified hyperparameter combinations. It will also retrain that model using the best hyperparameters for that model class. Let's see how to use this below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare variables to store the best model and score\n",
    "model_best_wine = None\n",
    "score_best_wine = 0.0\n",
    "\n",
    "# specify the number of folds for cross-validation\n",
    "K_folds = 5\n",
    "# specify the scoring method to use in cross-validation\n",
    "score = 'balanced_accuracy' # we'll use balanced accuracy since our classes are imbalanced\n",
    "# loop through the models using GridSearchCV to tune the hyperparameters\n",
    "for name, model in models.items():\n",
    "    print(f'Tuning {name} ...........................')\n",
    "    # create a GridSearchCV object for this model\n",
    "    grid_search = GridSearchCV(model, param_grid[name], cv=K_folds, scoring=score, refit=True, n_jobs=-1)\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    print(f'Best parameters: {grid_search.best_params_}')\n",
    "    print(f'Best validation score: {grid_search.best_score_}')\n",
    "    # check if this model is the best so far\n",
    "    if grid_search.best_score_ > score_best_wine:\n",
    "        print(f'*******New best model found: {name}')\n",
    "        score_best_wine = grid_search.best_score_\n",
    "        model_best_wine = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wine Data Test Set Evaluation\n",
    "Now that we have compared all the model and hyperparameter combinations that we are going to consider, we can examine model performance on the test set. \n",
    "\n",
    "First, let's get the predictions for the test set using the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's evaluate the best model on the test set\n",
    "# the grid search object has already refit the best model on the training set\n",
    "# let's get the predictions on the test set\n",
    "predictions = model_best_wine.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get the classification report\n",
    "print(classification_report(y_test, predictions, target_names=[f'Class {i}' for i in range(3)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get the confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=[f'Class {i}' for i in range(3)])\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title('Confusion Matrix For Wine Data Using Best Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the final model does extremely well on the test data. This is certainly not the normal situation on real-world problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 Iris Data Set\n",
    "Let's repeat the process from above on another data set. Here we load the Iris dataset which was first published by the famous statistician, Sir R.A. Fisher. Per the scikit-learn [documentation](https://scikit-learn.org/stable/datasets/toy_dataset.html, \"_The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other_\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the iris dataset from sklearn\n",
    "iris = load_iris()\n",
    "\n",
    "# the iris object is a dictionary-like object NOT a pandas dataframe\n",
    "print(iris.DESCR)\n",
    "\n",
    "# we can work with the wine object directly, but let's convert it to a pandas dataframe to be consistent with our prior examples\n",
    "df_iris = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df_iris['target'] = iris.target\n",
    "display(df_iris.head())\n",
    "print(df_iris.target.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again take a quick look at the feature distributions within the iris classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_box_grid_by_target(df_iris, 'target', num_cols=4, fig_size=(12,4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We (okay, I) would like to make this problem a little more difficult. So, in the cell block below, we are going to add noise to the data features. We would would (almost) never do this (there are some exceptions in deep learning). We're doing it here to make sure the model doesn't perform perfectly on the test set so we can illustrate performance evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's shuffle the wine data\n",
    "df_iris = df_iris.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "# add gaussian noise to the iris data to make it more interesting\n",
    "np.random.seed(SEED)\n",
    "df_iris.iloc[:, :-1] += np.random.normal(0, 0.4, df_iris.iloc[:, :-1].shape)\n",
    "\n",
    "# let's split the df_iris data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_iris.drop('target', axis=1), df_iris.target, test_size=0.3, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now follow the same steps as before. First, we standardize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's standardize the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit the scaler to the training data\n",
    "scaler.fit(X_train)\n",
    "# transform the training data and put it into a new dataframe\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "\n",
    "# transform the testing data and put it into a new dataframe\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "# show the scaled training data\n",
    "display(X_train_scaled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the same models and hyperparameter combinations as we used for the wine dataset (specified above). We again perform a systematic approach using grid search and cross validation to determine the best model for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_best_iris = None\n",
    "score_best_iris = 0.0\n",
    "K_folds = 5\n",
    "score = 'accuracy' # we'll use accuracy since our classes are balanced\n",
    "for name, model in models.items():\n",
    "    print(f'Tuning {name} ...........................')\n",
    "    grid_search = GridSearchCV(model, param_grid[name], cv=K_folds, scoring=score, refit=True, n_jobs=-1)\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    print(f'Best parameters: {grid_search.best_params_}')\n",
    "    print(f'Best validation score: {grid_search.best_score_}')\n",
    "    if grid_search.best_score_ > score_best_iris:\n",
    "        print(f'*******New best model found: {name}')\n",
    "        score_best_iris = grid_search.best_score_\n",
    "        model_best_iris = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the predictions and plot the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's evaluate the best model on the test set\n",
    "# the grid search object has already refit the best model on the training set\n",
    "# let's get the predictions on the test set\n",
    "predictions = model_best_iris.predict(X_test_scaled)\n",
    "\n",
    "# let's get the confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=[f'Class {i}' for i in range(3)])\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title('Confusion Matrix For Iris Data Using Best Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the best model performs reasonably well on the test data, but not perfectly - though if we remove the intentionally added noise, it will do much better. Let's look at the classification report to get the point metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get the classification report\n",
    "print(classification_report(y_test, predictions, target_names=[f'Class {i}' for i in range(3)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also look at the Reciever Operating Characteristic Curves (ROC) and the Area Under the Curves (AUC). Recall, that the ROC let's us examine what happens as we change the decision threshold for assigning a sample to a class based on the probability of the sample belonging to that class as predicted by the model. The ROC and AUC metrics are typically computed for binary outcomes. In this example, which is a multi-class scenario, we need to address this in order to create the ROC. We will consider each class separately. That is, we will create an ROC for each class for which we will treat the class of interest as the _positive_ class and all other classes as the _negative_ class. This will give us one ROC for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot the roc curve for each class\n",
    "# we need to get the probabilities for each class\n",
    "probs = model_best_iris.predict_proba(X_test_scaled)\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = {0:0, 1:0, 2:0}\n",
    "for i in range(3):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test, probs[:, i], pos_label=i)\n",
    "    # roc_auc expects binary values, so we need to convert the target to binary\n",
    "    # we're using one vs. all, so we'll convert the target to 1 for the class we're interested in and 0 for the other classes\n",
    "    y_test_i = np.array(y_test == i).astype(int)\n",
    "    roc_auc[i] = roc_auc_score(y_test_i, probs[:, i])\n",
    "    plt.plot(fpr[i], tpr[i], label=f'Class {i} (AUC = {roc_auc[i]:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve For Iris Data Using Best Model')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's look at the learning curves. The learning curves examine the the model performance as a function of the size of the training data. This analysis can help determine if our model is affected by bias, variance (overfitting), or both. The learning curve graph contains two line plots: (1) training - this represents the model performance on the training data with different training set sizes. (2) validation - this represents the model performance on a held out validation set with different training sets. There are a few key things we can interpret from learning curves:\n",
    "1. If the training score is <1 for the largets available training set size, the model may suffer from bias (insufficient capacity) \n",
    "2. If the the training and validation score are different for the largest values (e.g, the validation socre is less than the training score), the model is likely affected by variance (overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot the learning curve for the best model\n",
    "# we'll use the balanced accuracy as our scoring metric\n",
    "train_sizes, train_scores, test_scores = learning_curve(model_best_iris, X_train_scaled, y_train, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# let's create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                 color=\"r\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                    test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "                label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "                label=\"Cross-validation score\")\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Learning Curve For Iris Data Using Best Model\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpsc-4300",
   "language": "python",
   "name": "cpsc-4300"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
