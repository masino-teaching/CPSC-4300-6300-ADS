{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='heading'>\n",
    "    <div style='float:left;'><h1>CPSC 4300/6300: Applied Data Science</h1></div>\n",
    "     <img style=\"float: right; padding-right: 10px\" width=\"100\" src=\"https://raw.githubusercontent.com/bsethwalker/clemson-cs4300/main/images/clemson_paw.png\"> </div>\n",
    "     </div>\n",
    "\n",
    "**Clemson University**<br>\n",
    "**Instructor(s):** Aaron Masino <br>\n",
    "\n",
    "## Lab 9A: Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab will introduce you to the creation, training, and evaluation of recurrent neural network models using the [PyTorch](https://pytorch.org/) and [PyTorch Lightning](https://lightning.ai/pytorch-lightning) libraries. PyTorch contains core capabilities related to the development of deep learning models. PyTorch Lightning provides functionally that abstracts much of the process of training and evaluating deep learning models created with PyTorch.\n",
    "\n",
    "By the end of this lab, you should be able to:\n",
    "- Evaluate LSTM output dimensions using PyTorch LSTM\n",
    "- Apply PyTorch LSTM and LSTM concepts to build a text sequence labeling model\n",
    "- Evaluate text sequence labeling model perforamnce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as F \n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch import seed_everything\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "import torchmetrics as TM\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_directory(path: str):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "dir_dataroot = os.path.join(\"..\", \"data\")\n",
    "create_data_directory(dir_dataroot)\n",
    "\n",
    "dir_lightning = os.path.join(\"..\", \"lightning\")\n",
    "create_data_directory(dir_lightning)\n",
    "\n",
    "SEED = 123456"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following cell to set the `device` variable to the appropriate accelerator for your environment. NOTE: this lab notebook will run extremely slowly without GPU support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device variable to cuda or mps if available otherwise cpu\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 Sequence Labelling with Recurrent Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now work with sequence data using the PyTorch `lstm` module which implements the Long Short Term Memory (LSTM) architecture. First, we need to understand the dimension semantics used in PyTorch for data passed into an LSTM cell. The figure below illustrates the expected arrangement Here, we consider the sequence to be\n",
    "text consisting of a sequence of words. Each word is represented by a set of numbers (the features), often called an __embedding__. <br/><br/>\n",
    "<img src=\"https://github.com/masino-teaching/CPSC-4300-6300-ADS/blob/main/images/lstm-pytorch-data-structure.png?raw=true\" style=\"background-color:white;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by passing some Tensors through an `lstm` block to get a better understanding of the data arrangement, the inner workings of the lstm, and the lstm outoupt. Let's start by processing a _single_ sequence. First, we'll process one __element__ in the sequence at a time. Then we'll process the entire sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dim = 10 #the number of features used to represent an element in the sequence, e.g., a word in a sentence\n",
    "hidden_dim = 20 #the number of features in the hidden state\n",
    "seq_len = 5 #the length of the sequence, for PyTorch every sequence in the batch must have the same length\n",
    "num_sequences = 1 #the number of sequences in the batch\n",
    "\n",
    "lstm = nn.LSTM(input_size=feature_dim, hidden_size=hidden_dim)\n",
    "\n",
    "# create random input data\n",
    "inputs = torch.randn(seq_len, num_sequences, feature_dim)\n",
    "\n",
    "# let's process one element in the sequence at a time\n",
    "print(\"Processing one element at a time\")\n",
    "for i in range(seq_len):\n",
    "    out, (cn,hn) = lstm(inputs[i].view(1, num_sequences, feature_dim)) # view is necessary to add a batch dimension after selecting the sequence elements\n",
    "    print(f\"Step {i}: out.shape:{out.shape}, hn.shape:{hn.shape}, cn.shape:{cn.shape}\")\n",
    "\n",
    "# Let's process the entire sequence at once\n",
    "print(\"\\nProcessing the entire sequence at once\")\n",
    "out, (hn,cn) = lstm(inputs)\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "print(f\"Hidden state shape: {hn.shape}\")\n",
    "print(f\"Cell state shape: {cn.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's process __multiple__ sequences. First, we'll process one __sequence__ at a time. Then we'll process the entire sequence as a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dim = 10 #the number of features used to represent an element in the sequence, e.g., a word in a sentence\n",
    "hidden_dim = 20 #the number of features in the hidden state\n",
    "seq_len = 5 #the length of the sequence, for PyTorch every sequence in the batch must have the same length\n",
    "num_sequences = 3 #the number of sequences in the batch\n",
    "\n",
    "lstm = nn.LSTM(input_size=feature_dim, hidden_size=hidden_dim)\n",
    "\n",
    "# create random input data\n",
    "inputs = torch.randn(seq_len, num_sequences, feature_dim)\n",
    "\n",
    "# let's process one sequence in the batch at a time\n",
    "print(\"Processing one sequence at a time\")\n",
    "for i in range(num_sequences):\n",
    "    out, (hn,cn) = lstm(inputs[:,i,:].view(seq_len, 1, feature_dim)) # unsqueeze is necessary to add a batch dimension after selecting the sequence\n",
    "    print(f\"Step {i}: out.shape:{out.shape}, hn.shape:{hn.shape}, cn.shape:{cn.shape}\")\n",
    "\n",
    "# let's process the entire batch\n",
    "print(\"\\nProcessing the entire batch\")\n",
    "out, (hn, cn) = lstm(inputs)\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "print(f\"Hidden state shape: {hn.shape}\")\n",
    "print(f\"Cell state shape: {cn.shape}\")\n",
    "print(f\"hidden squeeze shape: {hn.squeeze(0).shape}\")\n",
    "\n",
    "# What if we pass them through a linear layer for classification?\n",
    "num_tags = 4\n",
    "ll = nn.Linear(hidden_dim, num_tags)\n",
    "tags_all = ll(out)\n",
    "tags_last_only = ll(hn.squeeze(0))\n",
    "print(f\"Tags all shape: {tags_all.shape}\")\n",
    "print(f\"Tags last only shape: {tags_last_only.shape}\")\n",
    "print(tags_last_only)\n",
    "print(tags_all[seq_len-1,:,:])\n",
    "print(tags_all[seq_len-1,:,:].squeeze(0).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging\n",
    "Now we are going to use the `lstm` module to build a _part of speech_ (POS) tagger. The POS tagger will take an input sequence of words forming a sentence and tag each word with its POS (e.g., noun). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "We will be using a corpus of data stored in Pandas DataFrame loaded from hugging face, see [here](https://huggingface.co/datasets/batterydata/pos_tagging) for more information. The DataFrame has two columns, `words` and `labels`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"hf://datasets/batterydata/pos_tagging/train.json\", lines=True)\n",
    "display(df.head())\n",
    "print(df.shape)\n",
    "# get the unique tags\n",
    "tags = df['labels'].explode().unique()\n",
    "print(tags)\n",
    "print(len(tags))\n",
    "# get the most frequent tags\n",
    "tag_counts = df['labels'].explode().value_counts()\n",
    "print(tag_counts[0:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it easier to work with the data, we will create a `DataSet` class that represents the sequence of words and tags as indices from a fixed vocabulary. (More on that below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PandasIndexedTextDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, text_indicies_column, target_column):\n",
    "        self.x = df[text_indicies_column]\n",
    "        self.t = df[target_column]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.Tensor(self.x.iloc[idx]).int(), torch.Tensor(self.t.iloc[idx]).int()\n",
    "    \n",
    "x = [[1,2,-1,-1], [5,6,7,-1], [1,5,8,9]]\n",
    "t = [[0,0,-1,-1], [1,1,0,-1], [1,0,1,0]]\n",
    "ds = PandasIndexedTextDataSet(pd.DataFrame({'word_indices': x, 'target': t}), 'word_indices', 'target')\n",
    "\n",
    "dl = DataLoader(ds, batch_size=2, shuffle=False)\n",
    "xin, tin = next(iter(dl))\n",
    "print(type(xin), type(tin))\n",
    "print(xin)\n",
    "print(xin.shape)\n",
    "print(tin)\n",
    "print(tin.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with text data, we need to represent each word (and tag) as a number. We will initially do this by mapping each unique word that occurs in our dataset to an integer value. Then, we can reprsent a sequence of words by a list of integers, each corresponding to the mapped integer from the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexedCorpus():\n",
    "    def __init__(self, df, column):\n",
    "        self.df = df\n",
    "        self.column = column\n",
    "        self.word2idx, self.idx2word = self.build_indices()\n",
    "        self.max_len = self.get_max_len()\n",
    "    \n",
    "    def build_indices(self):\n",
    "        word2idx = {}\n",
    "        idx2word = {}\n",
    "        idx = 0\n",
    "        for word_list in self.df[self.column]:\n",
    "            for word in word_list:\n",
    "                if word.lower() not in word2idx:\n",
    "                    word2idx[word.lower()] = idx\n",
    "                    idx2word[idx] = word.lower()\n",
    "                    idx += 1\n",
    "        word2idx['<PAD>'] = idx\n",
    "        idx2word[idx] = '<PAD>'\n",
    "        word2idx['<UNK>'] = idx+1\n",
    "        idx2word[idx+1] = '<UNK>'\n",
    "        return word2idx, idx2word\n",
    "    \n",
    "    def vocab_size(self):\n",
    "        return len(self.word2idx)\n",
    "    \n",
    "    def get_max_len(self):\n",
    "        return max([len(word_list) for word_list in self.df[self.column]])\n",
    "    \n",
    "    def words2indices(self, words, add_padding=True):\n",
    "        indices = [self.word2idx.get(word.lower(), self.word2idx['<UNK>']) for word in words]\n",
    "        if add_padding:\n",
    "            indices += [self.word2idx['<PAD>']] * (self.max_len - len(indices))\n",
    "        if len(indices) > self.max_len:\n",
    "            indices = indices[:self.max_len]\n",
    "        return indices\n",
    "    \n",
    "    def wordBatch2IndicesBatch(self, word_batch):\n",
    "        return [self.words2indices(words) for words in word_batch]\n",
    "    \n",
    "df = pd.DataFrame({'words': [['The', 'dog', 'ran'], ['The', 'boy', 'jumped','high']], 'target':[['N', 'V', 'NN'], ['V', 'NN', 'V', 'ADJ']]})\n",
    "icw = IndexedCorpus(df, 'words')\n",
    "print(icw.word2idx)\n",
    "print(icw.words2indices(df.words.iloc[0]))\n",
    "print(icw.words2indices(['The', 'dog', 'ran', 'fast']))\n",
    "print(icw.wordBatch2IndicesBatch(df.words))\n",
    "\n",
    "ict = IndexedCorpus(df, 'target')\n",
    "print(ict.word2idx)\n",
    "print(ict.wordBatch2IndicesBatch(df.target))\n",
    "df_idx = pd.DataFrame({'words': icw.wordBatch2IndicesBatch(df.words), 'target': ict.wordBatch2IndicesBatch(df.target)})\n",
    "df_idx\n",
    "len(df_idx.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a PyTorch LightningDataModule to handle converting our samples to their intger representations and data loading for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PandasIndexedTextDataModule(L.LightningDataModule):\n",
    "    def __init__(self, df, text_column, target_column, batch_size=32, val_split=0.2, test_split=0.1):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.text_column = text_column\n",
    "        self.target_column = target_column\n",
    "        self.batch_size = batch_size\n",
    "        self.val_split = val_split\n",
    "        self.test_split = test_split\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        n = len(self.df.index)\n",
    "        val_size = int(n * self.val_split)\n",
    "        test_size = int(n * self.test_split)\n",
    "        train_size = n - val_size - test_size\n",
    "\n",
    "        # split the data\n",
    "        self.train = self.df.iloc[:train_size]\n",
    "        self.val = self.df.iloc[train_size:train_size+val_size]\n",
    "        self.test = self.df.iloc[train_size+val_size:]\n",
    "\n",
    "        # create the IndexdCorpus objects using the train data\n",
    "        self.icw = IndexedCorpus(self.df, self.text_column)\n",
    "        self.vocab_size = self.icw.vocab_size()\n",
    "        self.ict = IndexedCorpus(self.df, self.target_column)\n",
    "        self.num_classes = self.ict.vocab_size()\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        ds = PandasIndexedTextDataSet(pd.DataFrame({'words': self.icw.wordBatch2IndicesBatch(self.train[self.text_column]), \n",
    "                                                    'target': self.ict.wordBatch2IndicesBatch(self.train[self.target_column])}), 'words', 'target')\n",
    "        return DataLoader(ds, batch_size=self.batch_size, shuffle=False)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        ds = PandasIndexedTextDataSet(pd.DataFrame({'words': self.icw.wordBatch2IndicesBatch(self.val[self.text_column]), \n",
    "                                                    'target': self.ict.wordBatch2IndicesBatch(self.val[self.target_column])}), 'words', 'target')\n",
    "        return DataLoader(ds, batch_size=self.batch_size, shuffle=False)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        ds = PandasIndexedTextDataSet(pd.DataFrame({'words': self.icw.wordBatch2IndicesBatch(self.test[self.text_column]), \n",
    "                                                    'target': self.ict.wordBatch2IndicesBatch(self.test[self.target_column])}), 'words', 'target')\n",
    "        return DataLoader(ds, batch_size=self.batch_size, shuffle=False)\n",
    "    \n",
    "df = pd.DataFrame({'words': [['The', 'dog', 'ran'], \n",
    "                             ['The', 'boy', 'jumped','high'],\n",
    "                             ['A', 'dog', 'ran'],\n",
    "                             ['The', 'dog', 'and', 'boy', 'ran']], \n",
    "                  'target':[['N', 'V', 'NN'], \n",
    "                            ['V', 'NN', 'V', 'ADJ'],\n",
    "                            ['V', 'N', 'NN'],\n",
    "                            ['N', 'V', 'CC', 'N', 'V']]})\n",
    "dm = PandasIndexedTextDataModule(df, 'words', 'target', batch_size=2, val_split=0.25, test_split=0.25)\n",
    "dm.setup()\n",
    "print(next(iter(dm.train_dataloader())))\n",
    "print(next(iter(dm.val_dataloader())))\n",
    "print(next(iter(dm.test_dataloader())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's be careful with our dimensions. We'll see in our model's forward method that we need to permute the input tensor\n",
    "# to get the the sequence length in the first dimension, the batch size in the second dimension,\n",
    "# and the feature dimension in the third dimension\n",
    "dm = PandasIndexedTextDataModule(df, 'words', 'target', batch_size=2, val_split=0.25, test_split=0.25)\n",
    "dm.setup()\n",
    "batch = next(iter(dm.train_dataloader()))\n",
    "print(batch[0].shape)\n",
    "print(len(batch))\n",
    "embedding_dim = 10\n",
    "we = nn.Embedding(dm.icw.vocab_size(), embedding_dim)\n",
    "embedding = we(batch[0].permute(1,0))\n",
    "print(embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Tagging Model\n",
    "We are not ready to build our POS tagger. We will again use the PyTorch LightningModule. This implementation requires many changes from our previous models to account for the fact that we're modeling sequence data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(L.LightningModule):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, num_tags):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, num_tags)\n",
    "        self.num_tags = num_tags\n",
    "\n",
    "        # validation metrics - we will use these to compute the metrics at the end of the validation epoch\n",
    "        self.val_metrics_tracker = TM.wrappers.MetricTracker(TM.MetricCollection([TM.classification.MulticlassAccuracy(num_classes=num_tags)]), maximize=True)\n",
    "        self.validation_step_predictions = []\n",
    "        self.validation_step_targets = []\n",
    "        self.val_losses = []\n",
    "\n",
    "        # test metrics - we will use these to compute the metrics at the end of the test epoch\n",
    "        self.test_roc = TM.ROC(task=\"multiclass\", num_classes=num_tags) # roc and cm have methods we want to call so store them in a variable\n",
    "        self.test_cm = TM.ConfusionMatrix(task='multiclass', num_classes=num_tags)\n",
    "        self.test_metrics_tracker = TM.wrappers.MetricTracker(TM.MetricCollection([TM.classification.MulticlassAccuracy(num_classes=num_tags), \n",
    "                                                            self.test_roc, self.test_cm]), maximize=True)\n",
    "        \n",
    "        # test outputs and targets - we will store the outputs and targets for the test step\n",
    "        self.test_step_outputs = []\n",
    "        self.test_step_targets = []\n",
    "        self.test_losses = []\n",
    "\n",
    "    def forward(self, X):\n",
    "        embeds = self.word_embeddings(X.permute(1,0))\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        tag_space = self.hidden2tag(lstm_out)\n",
    "        tag_scores = nn.functional.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        sentences, tags = batch\n",
    "        tag_scores = self(sentences)\n",
    "        loss = nn.functional.nll_loss(tag_scores.permute(1,0,2).reshape(-1, self.num_tags), tags.flatten().long())\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        sentences, tags = batch\n",
    "        tag_scores = self(sentences)\n",
    "        loss = nn.functional.nll_loss(tag_scores.permute(1,0,2).reshape(-1, self.num_tags), tags.flatten().long())\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True)\n",
    "\n",
    "        # store the outputs and targets for the epoch end step\n",
    "        self.validation_step_predictions.append(tag_scores.permute(1,0,2).reshape(-1, self.num_tags))\n",
    "        self.validation_step_targets.append(tags.flatten().long())\n",
    "        self.val_losses.append(loss)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        sentences, tags = batch\n",
    "        tag_scores = self(sentences)\n",
    "        loss = nn.functional.nll_loss(tag_scores.permute(1,0,2).reshape(-1, self.num_tags), tags.flatten().long())\n",
    "        self.log('test_loss', loss, on_step=True, on_epoch=True)\n",
    "\n",
    "        # store the outputs and targets for the epoch end step\n",
    "        self.test_step_outputs.append(tag_scores.permute(1,0,2).reshape(-1, self.num_tags))\n",
    "        self.test_step_targets.append(tags.flatten().long())\n",
    "        self.test_losses.append(loss)\n",
    "\n",
    "    # at the end of the epoch compute the metrics\n",
    "    def on_validation_epoch_end(self):\n",
    "        # stack all the outputs and targets into a single tensor\n",
    "        all_preds = torch.vstack(self.validation_step_predictions)\n",
    "        all_targets = torch.hstack(self.validation_step_targets)\n",
    "        \n",
    "        # compute the metrics\n",
    "        loss = nn.functional.nll_loss(all_preds, all_targets)\n",
    "        self.val_metrics_tracker.increment()\n",
    "        self.val_metrics_tracker.update(all_preds, all_targets)\n",
    "        self.log('val_loss_epoch_end', np.mean([_.cpu().item() for _ in self.val_losses]))\n",
    "        \n",
    "        # clear the validation step outputs\n",
    "        self.validation_step_predictions.clear()\n",
    "        self.validation_step_targets.clear()\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        all_preds = torch.vstack(self.test_step_outputs)\n",
    "        all_targets = torch.hstack(self.test_step_targets)\n",
    "        \n",
    "        self.test_metrics_tracker.increment()\n",
    "        self.test_metrics_tracker.update(all_preds, all_targets)\n",
    "        # clear the test step outputs\n",
    "        self.test_step_outputs.clear()\n",
    "        self.test_step_targets.clear()\n",
    "        self.log('test_loss_epoch_end', np.mean([_.cpu().item() for _ in self.test_losses]))\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(SEED)\n",
    "df = pd.read_json(\"hf://datasets/batterydata/pos_tagging/train.json\", lines=True)\n",
    "dm = PandasIndexedTextDataModule(df, 'words', 'labels', batch_size=32, val_split=0.2, test_split=0.1)\n",
    "dm.setup()\n",
    "\n",
    "embedding_dim = 10\n",
    "hidden_dim = 64\n",
    "model = LSTMTagger(embedding_dim, hidden_dim, dm.vocab_size, dm.num_classes)\n",
    "\n",
    "trainer = L.Trainer(default_root_dir=dir_lightning,\n",
    "                    max_epochs=3,\n",
    "                    callbacks=[EarlyStopping(monitor=\"val_loss_epoch_end\", mode=\"min\", patience=3)])\n",
    "trainer.fit(model=model, train_dataloaders=dm.train_dataloader(), val_dataloaders=dm.val_dataloader())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examing the validation set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mca = model.val_metrics_tracker.compute_all()['MulticlassAccuracy']\n",
    "plt.plot(range(1, len(mca)+1), mca, marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Epoch Validation Accuracy')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's examine the test set performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model=model, dataloaders=dm.test_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rslt = model.test_metrics_tracker.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = rslt['MulticlassROC']\n",
    "indices = [dm.ict.word2idx[idx.lower()] for idx in tag_counts.index[0:10]]\n",
    "for i in indices:\n",
    "    plt.plot(fpr[i], tpr[i], label=f'{dm.ict.idx2word[i]}')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [dm.ict.word2idx[idx.lower()] for idx in tag_counts.index[-10:]]\n",
    "for i in indices:\n",
    "    word = dm.ict.idx2word[i]\n",
    "    plt.plot(fpr[i], tpr[i], label=f'{dm.ict.idx2word[i]}')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")   #\"cuda:0\"\n",
    "# put the model in evaluation mode so that the parameters are fixed and we don't compute gradients\n",
    "model.eval()\n",
    "y_true=[]\n",
    "y_pred=[]\n",
    "# use torch.no_grad() to disable gradient computation\n",
    "\n",
    "with torch.no_grad():\n",
    "    # iterate over the test loader minibatches\n",
    "    for test_data in dm.test_dataloader():\n",
    "        sentences, tags = test_data\n",
    "        tag_scores = model(sentences)\n",
    "        sentences.to(device)\n",
    "        tags = tags.flatten()\n",
    "        tags.to(device)\n",
    "        # store the outputs and targets for the epoch end step\n",
    "        pred = tag_scores.permute(1,0,2).reshape(-1, model.num_tags).argmax(dim=1)\n",
    "        # get the images and labels from the test loader and move them to the cpu. this will make it easier to use them with sklearn\n",
    "        for i in range(len(pred)):\n",
    "            y_true.append(tags[i].item())\n",
    "            y_pred.append(pred[i].item())\n",
    "\n",
    "print(classification_report(y_true,y_pred,target_names=dm.ict.word2idx.keys(),digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "4300-pytorch",
   "language": "python",
   "name": "cpsc4300torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
